"""
Metrics calculation for CCAD Framework.

This module provides domain-agnostic metrics for evaluating the 
performance of hypothesis generation systems using CCAD.

Key metrics:
    - UPT (Unique-per-Token): Token efficiency
    - Coverage: Diversity of generated space
    - Redundancy: Duplicate rate
    - Saturation: Rate of diminishing returns
"""

from typing import List, Dict, Any, Callable, Optional
import statistics


def calculate_upt(total_generated: int, unique_accepted: int) -> float:
    """
    Calculate Unique-per-Token (UPT) efficiency.
    
    UPT measures how many unique items you get per generated item.
    Higher is better (less waste).
    
    Args:
        total_generated: Total number of items generated by LLM
        unique_accepted: Number of unique items after deduplication
    
    Returns:
        UPT ratio in [0, 1]
        - 1.0 = perfect (no duplicates)
        - 0.5 = 50% efficiency
        - <0.3 = poor efficiency
    
    Examples:
        >>> calculate_upt(1000, 600)
        0.6
        
        >>> calculate_upt(500, 500)
        1.0  # Perfect efficiency
    """
    if total_generated == 0:
        return 0.0
    return unique_accepted / total_generated


def calculate_duplicate_rate(
    duplicates_filtered: int, 
    total_generated: int
) -> float:
    """
    Calculate duplicate rate (inverse of UPT).
    
    Args:
        duplicates_filtered: Number of duplicates removed
        total_generated: Total items generated
    
    Returns:
        Duplicate rate in [0, 1]
        - 0.0 = no duplicates
        - 0.5 = half were duplicates
        - >0.7 = high redundancy (saturation)
    
    Examples:
        >>> calculate_duplicate_rate(400, 1000)
        0.4
    """
    if total_generated == 0:
        return 0.0
    return duplicates_filtered / total_generated


def calculate_coverage(
    items: List[Any],
    clustering_fn: Callable[[List[Any]], int],
    reference_size: Optional[int] = None
) -> Dict[str, Any]:
    """
    Calculate coverage metrics (how much of the space is explored).
    
    Args:
        items: List of generated items
        clustering_fn: Function that clusters items and returns cluster count
        reference_size: Optional size of reference/total space for normalization
    
    Returns:
        Dictionary with coverage metrics:
            - n_items: Total items
            - n_clusters: Number of distinct clusters/regions
            - avg_per_cluster: Average items per cluster
            - coverage_ratio: n_clusters / reference_size (if provided)
    
    Examples:
        >>> def count_scaffolds(mols): 
        ...     return len(set(get_scaffold(m) for m in mols))
        >>> calculate_coverage(molecules, count_scaffolds)
        {
            'n_items': 500,
            'n_clusters': 87,
            'avg_per_cluster': 5.7,
            'coverage_ratio': None
        }
    """
    n_items = len(items)
    n_clusters = clustering_fn(items) if items else 0
    avg_per_cluster = n_items / n_clusters if n_clusters > 0 else 0.0
    coverage_ratio = n_clusters / reference_size if reference_size else None
    
    return {
        'n_items': n_items,
        'n_clusters': n_clusters,
        'avg_per_cluster': avg_per_cluster,
        'coverage_ratio': coverage_ratio
    }


def calculate_saturation_score(duplicate_rates: List[float]) -> float:
    """
    Calculate saturation score (how quickly generation is exhausting the space).
    
    Looks at trend in duplicate rates across chunks:
        - Rising trend → Space is saturating
        - Flat/declining → Still exploring new regions
    
    Args:
        duplicate_rates: List of duplicate rates per chunk (chronological)
    
    Returns:
        Saturation score in [0, 1]:
            - 0.0 = no saturation (rates stable/declining)
            - 0.5 = moderate saturation
            - >0.8 = high saturation (should stop)
    
    Examples:
        >>> calculate_saturation_score([0.2, 0.3, 0.4, 0.6, 0.8])
        0.85  # Clear upward trend
        
        >>> calculate_saturation_score([0.3, 0.2, 0.4, 0.3, 0.2])
        0.15  # No trend, still exploring
    """
    if len(duplicate_rates) < 3:
        return 0.0
    
    # Simple linear regression: are rates increasing?
    n = len(duplicate_rates)
    x = list(range(n))
    mean_x = statistics.mean(x)
    mean_y = statistics.mean(duplicate_rates)
    
    numerator = sum((x[i] - mean_x) * (duplicate_rates[i] - mean_y) for i in range(n))
    denominator = sum((x[i] - mean_x) ** 2 for i in range(n))
    
    if denominator == 0:
        return 0.0
    
    slope = numerator / denominator
    
    # Normalize slope to [0, 1]
    # Positive slope → saturation
    # Negative/zero slope → still exploring
    saturation = max(0.0, min(1.0, slope * 5))  # Scale factor 5 is heuristic
    
    return saturation


def calculate_peak_efficiency(duplicate_rates: List[float]) -> float:
    """
    Calculate peak efficiency (best chunk performance).
    
    Identifies the chunk with lowest duplicate rate.
    Useful for understanding maximum achievable efficiency.
    
    Args:
        duplicate_rates: List of duplicate rates per chunk
    
    Returns:
        Peak efficiency (1 - min_duplicate_rate)
    
    Examples:
        >>> calculate_peak_efficiency([0.6, 0.4, 0.2, 0.5, 0.7])
        0.8  # Best chunk had 20% duplicates → 80% efficiency
    """
    if not duplicate_rates:
        return 0.0
    
    min_dup_rate = min(duplicate_rates)
    return 1.0 - min_dup_rate


def generate_summary_report(metrics: Dict[str, Any]) -> str:
    """
    Generate human-readable summary report from metrics.
    
    Args:
        metrics: Dictionary containing CCAD metrics:
            - total_generated
            - unique_accepted
            - duplicates_filtered
            - chunks_processed
            - duplicate_rates (list)
    
    Returns:
        Formatted multi-line summary string
    
    Examples:
        >>> print(generate_summary_report(metrics))
        ╔═══════════════════════════════════════════════════════════════╗
        ║              CCAD PERFORMANCE SUMMARY                         ║
        ╚═══════════════════════════════════════════════════════════════╝
        
        Generation Metrics:
          • Chunks processed:      24
          • Total generated:       884
          • Unique accepted:       500
          • Duplicates filtered:   384
        
        Efficiency Metrics:
          • Token efficiency (UPT): 56.6%
          • Avg duplicate rate:     38.6%
          • Peak efficiency:        83.7% (chunk 14)
        
        Saturation:
          • Saturation score:       0.42 (Moderate)
          • Recommendation:         Continue generation
        
        Performance: ★★★★☆ (Very Good)
    """
    total_gen = metrics.get('total_generated', 0)
    unique = metrics.get('unique_accepted', 0)
    dups = metrics.get('duplicates_filtered', 0)
    chunks = metrics.get('chunks_processed', 0)
    dup_rates = metrics.get('duplicate_rates', [])
    
    upt = calculate_upt(total_gen, unique)
    avg_dup_rate = statistics.mean(dup_rates) if dup_rates else 0.0
    peak_eff = calculate_peak_efficiency(dup_rates)
    sat_score = calculate_saturation_score(dup_rates)
    
    # Determine saturation status
    if sat_score < 0.3:
        sat_status = "Low"
        sat_color = "green"
        recommendation = "Continue generation"
    elif sat_score < 0.6:
        sat_status = "Moderate"
        sat_color = "yellow"
        recommendation = "Continue with monitoring"
    else:
        sat_status = "High"
        sat_color = "red"
        recommendation = "Consider stopping (diminishing returns)"
    
    # Overall performance rating
    if upt > 0.6:
        rating = "★★★★★ (Excellent)"
    elif upt > 0.5:
        rating = "★★★★☆ (Very Good)"
    elif upt > 0.4:
        rating = "★★★☆☆ (Good)"
    elif upt > 0.3:
        rating = "★★☆☆☆ (Fair)"
    else:
        rating = "★☆☆☆☆ (Poor)"
    
    report = f"""
╔═══════════════════════════════════════════════════════════════╗
║              CCAD PERFORMANCE SUMMARY                         ║
╚═══════════════════════════════════════════════════════════════╝

Generation Metrics:
  • Chunks processed:      {chunks}
  • Total generated:       {total_gen}
  • Unique accepted:       {unique}
  • Duplicates filtered:   {dups}

Efficiency Metrics:
  • Token efficiency (UPT): {upt:.1%}
  • Avg duplicate rate:     {avg_dup_rate:.1%}
  • Peak efficiency:        {peak_eff:.1%}

Saturation:
  • Saturation score:       {sat_score:.2f} ({sat_status})
  • Recommendation:         {recommendation}

Performance: {rating}

───────────────────────────────────────────────────────────────
Interpretation:
  • UPT > 50%: CCAD is working well
  • Peak efficiency: Best achievable in novel regions
  • Saturation: Rate of space exhaustion
───────────────────────────────────────────────────────────────
"""
    
    return report


def compare_baselines(
    ccad_metrics: Dict[str, Any],
    baseline_metrics: Dict[str, Any]
) -> str:
    """
    Compare CCAD performance vs baseline (naive generation).
    
    Args:
        ccad_metrics: Metrics from CCAD run
        baseline_metrics: Metrics from baseline run (no anti-dup)
    
    Returns:
        Formatted comparison report
    
    Examples:
        >>> print(compare_baselines(ccad_metrics, baseline_metrics))
        ╔═══════════════════════════════════════════════════════════════╗
        ║         CCAD vs BASELINE COMPARISON                           ║
        ╚═══════════════════════════════════════════════════════════════╝
        
        Metric                  Baseline    CCAD        Improvement
        ─────────────────────────────────────────────────────────────
        Token Efficiency (UPT)  31.2%       56.6%       +81.4%
        Duplicate Rate          68.8%       38.6%       -43.9%
        Unique per 1000 gen     312         566         +81.4%
        
        Summary: CCAD provides significant efficiency gains.
    """
    ccad_upt = calculate_upt(
        ccad_metrics['total_generated'],
        ccad_metrics['unique_accepted']
    )
    baseline_upt = calculate_upt(
        baseline_metrics['total_generated'],
        baseline_metrics['unique_accepted']
    )
    
    ccad_dup = calculate_duplicate_rate(
        ccad_metrics['duplicates_filtered'],
        ccad_metrics['total_generated']
    )
    baseline_dup = calculate_duplicate_rate(
        baseline_metrics['duplicates_filtered'],
        baseline_metrics['total_generated']
    )
    
    upt_improvement = ((ccad_upt - baseline_upt) / baseline_upt * 100) if baseline_upt > 0 else 0
    dup_improvement = ((baseline_dup - ccad_dup) / baseline_dup * 100) if baseline_dup > 0 else 0
    
    report = f"""
╔═══════════════════════════════════════════════════════════════╗
║         CCAD vs BASELINE COMPARISON                           ║
╚═══════════════════════════════════════════════════════════════╝

Metric                  Baseline    CCAD        Improvement
─────────────────────────────────────────────────────────────
Token Efficiency (UPT)  {baseline_upt:.1%}      {ccad_upt:.1%}      {upt_improvement:+.1f}%
Duplicate Rate          {baseline_dup:.1%}      {ccad_dup:.1%}      {dup_improvement:+.1f}%
Unique per 1000 gen     {int(baseline_upt * 1000)}         {int(ccad_upt * 1000)}         {upt_improvement:+.1f}%

───────────────────────────────────────────────────────────────
Summary: {'CCAD provides significant efficiency gains.' if upt_improvement > 30 else 'CCAD provides modest improvements.' if upt_improvement > 10 else 'CCAD shows minimal improvement.'}
───────────────────────────────────────────────────────────────
"""
    
    return report


class MetricsTracker:
    """
    Real-time metrics tracking during generation.
    
    Usage:
        tracker = MetricsTracker()
        for chunk in generation_loop:
            tracker.add_chunk(generated=50, unique=32, duplicates=18)
        
        print(tracker.get_summary())
    """
    
    def __init__(self):
        # Raw/valid generation counts
        self.total_raw_generated = 0
        self.total_valid_generated = 0
        # Backwards-compat aggregate (mapped to raw for reporting)
        self.total_generated = 0
        self.unique_accepted = 0
        self.duplicates_filtered = 0
        # legacy alias used in some tests/tools
        self.total_unique = 0
        self.chunks_processed = 0
        # duplicate_rates keep legacy meaning: among valid-generated
        self.duplicate_rates: List[float] = []
        self.duplicate_rates_raw: List[float] = []
        self.chunk_history: List[Dict[str, int]] = []
    
    def add_chunk(
        self,
        raw_generated: int,
        valid_generated: int,
        unique: int,
        duplicates: int
    ) -> None:
        """Add metrics from a single chunk."""
        self.total_raw_generated += raw_generated
        self.total_valid_generated += valid_generated
        # Maintain legacy total_generated mapped to raw for honest cost denominator
        self.total_generated = self.total_raw_generated
        self.unique_accepted += unique
        self.duplicates_filtered += duplicates
        # Keep legacy alias up-to-date
        self.total_unique = self.unique_accepted
        self.chunks_processed += 1
        
        dup_rate = duplicates / valid_generated if valid_generated > 0 else 0.0
        dup_rate_raw = duplicates / raw_generated if raw_generated > 0 else 0.0
        self.duplicate_rates.append(dup_rate)
        self.duplicate_rates_raw.append(dup_rate_raw)
        
        self.chunk_history.append({
            'raw_generated': raw_generated,
            'valid_generated': valid_generated,
            'unique': unique,
            'duplicates': duplicates,
            'dup_rate': dup_rate,
            'dup_rate_raw': dup_rate_raw
        })
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get current metrics dictionary."""
        return {
            # Backwards-compat: total_generated equals raw-generated
            'total_generated': self.total_generated,
            'total_raw_generated': self.total_raw_generated,
            'total_valid_generated': self.total_valid_generated,
            'unique_accepted': self.unique_accepted,
            'duplicates_filtered': self.duplicates_filtered,
            'chunks_processed': self.chunks_processed,
            'duplicate_rates': self.duplicate_rates,
            'duplicate_rates_raw': self.duplicate_rates_raw,
            'chunk_history': self.chunk_history
        }
    
    @property
    def total_unique(self) -> int:
        """Alias for unique_accepted (backwards compatibility for tests)."""
        return self.unique_accepted
    
    @total_unique.setter
    def total_unique(self, value: int) -> None:
        self.unique_accepted = int(value)
    
    def get_summary(self) -> str:
        """Get formatted summary report."""
        return generate_summary_report(self.get_metrics())
    
    def should_stop(self, saturation_threshold: float = 0.7) -> bool:
        """
        Recommend stopping based on saturation.
        
        Args:
            saturation_threshold: Stop if saturation exceeds this
        
        Returns:
            True if should stop, False otherwise
        """
        if self.chunks_processed < 5:
            return False  # Too early to judge
        
        sat_score = calculate_saturation_score(self.duplicate_rates)
        return sat_score > saturation_threshold

